{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ae32a3fd",
   "metadata": {},
   "source": [
    "# Basics of Mobile robotics - Project report"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb679180",
   "metadata": {},
   "source": [
    "### Students: Sushen Jilla Venkatesa 284402, Forero Philippe 284213, Mael Mouhoub 288287, Selina Bothner 284028"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7a6a93d",
   "metadata": {},
   "source": [
    "## 1.Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ed0bc79",
   "metadata": {},
   "source": [
    "We have realized this project in the framework of the course Basics of mobile robotics at EPFL in 2021. The goal of this project is to offer students the possibility to learn the basics of robotics, building a complex project within 4 weeks. We used a Thymio robot [1] and a webcam for this project. Our main constraints for this project were to integrate:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f8b6101",
   "metadata": {},
   "source": [
    "- Computer vision: Take a picture and do image processing to detect all elements of the map.\n",
    "-\tMotion control:  A Finite State Machine (FSM) with different modes, controlling the robot motors\n",
    "-\tGlobal and Local navigation: Find the shortest path to the goal, and avoids obstacles placed randomly while moving towards the goal \n",
    "-\tFiltering: diverse filters in Computer vision and Kalman filter in Motion control\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb780db6",
   "metadata": {},
   "source": [
    " We will combine these elements into a global project that will allow us to maneuver the Thymio robot in a defined map around obstacles. For more details, we will discuss the challenges linked to each part in the following sub-sections."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea16fb8b",
   "metadata": {},
   "source": [
    "## 2. The environment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a61d2325",
   "metadata": {},
   "source": [
    "We have created a special environment to run the project."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a56df63",
   "metadata": {},
   "source": [
    "![Map_real_1](images/Map_real_1.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8a2675d",
   "metadata": {},
   "source": [
    "This photo is taken with the webcam we use, and we can see here everything there is on the environment. We have covered the ground with cardboard to avoid ground pattern and false map border detection. To make the system as robust as possible, we also used a carboard on top of the webcam to avoid shadows from the overhead lights, as well as led lights to illuminate the map. This allows for the color detection used by the camera to be more robust as the lighting will always be about the same. We will go into more details of the choices of the colors for each item present on the map in the Vision module."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c85b5d5",
   "metadata": {},
   "source": [
    "## 3. Vision"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dec21cf3",
   "metadata": {},
   "source": [
    "The camera part uses mainly the OpenCV library.<br/><br/>\n",
    "The program starts off with the camera. It takes a picture and analyzes it. After every picture and every analysis, the user is asked if it seems okay. This is done to ensure a good map initialization, because sometimes the elements are not well detected. These issues arises if light conditions are not satisfied, or that the map is being out of bounds of camera field of view, etc.. <br/><br/>\n",
    "The camera part is divided in several parts: <br/>\n",
    "The outer border detection uses a picture taken by the camera, on which we apply a Canny filter[2]. This filter works by finding the region in an image where there is a strong change in intensity. This allows to detect contours in the image. We then sort through the different contours and keep the biggest one as this represents the border of the map. <br/>\n",
    "We must then crop this image around the outer contour. We perform a crop on the image and a rotation in the following way: <br/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f106d4b5",
   "metadata": {},
   "source": [
    "![MapTurn](images/MapTurn.jpg)\n",
    "![ImgMapTurn](images/ImgMapTurn.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f1e5982",
   "metadata": {},
   "source": [
    "This method was inspired by an example [3].<br/><br/>\n",
    "To make the algorithm work, it is imperative that that the ‘A’ corner is the one that is at the bottom of the image. If this is not the case the image will be rotated the other way. This is because we use a function from OpenCV that sets the lowest point as the first one, and then goes in a clockwise direction. As these points are then used to rotate the image, if they are not in the right order, the image will then be rotated in the other direction.<br/>\n",
    "Now that we have a reshaped image, we can start detecting the various elements we need to make the program work. Each of these elements are color coded, and all work relatively similarly. We used red to detect the obstacles, blue to detect the goal point and green to detect Thymio. We chose these colors as they are on opposite side of the spectrum from each other. <br/>\n",
    "In order to separate these colors, we use color masks, coded in HSV [4]. <br/>\n",
    "To detect the obstacles, we simply look for contours in the red mask. Noise and errors can be reduced by limiting the size of the obstacles detected. <br/>\n",
    "To detect the goal, we do the same thing but with the color blue. Once the goal is detected, we simply take the center of the rectangle as the goal point that will be used for the global navigation.\n",
    "The detection of the Thymio is also similar. The only difference, apart from the color is the fact that we have two rectangles to detect. This is done so that we can also determine the orientation of the robot. The center of the smallest rectangle (in the back) determines the position of the Thymio (the middle point between the wheels). We use the center of both rectangles to determine the angle of the robot. <br/>\n",
    "Once all these elements are detected, take the contours of the obstacles, and put them in a matrix that can be used by the global navigation. To put the obstacles into the matrix, we use a certain simplification. We find the four outer points and use them to create a square. This is not necessarily the most precise, but it is not problematic because we must anyways make the obstacles bigger, in order to take into account, the size of the Thymio.<br/> \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37e9bf88",
   "metadata": {},
   "source": [
    "![obstacleMatrix](images/obstacleMatrix.jpg)\n",
    "![BoxPoints](images/BoxPoints.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d54ad45e",
   "metadata": {},
   "source": [
    "To do these tasks, we used several examples and tutorials found on internet, such as this video [5]."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85ab6140",
   "metadata": {},
   "source": [
    "## 4.Global navigation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4296db37",
   "metadata": {},
   "source": [
    "The global navigation starts after the analysis of the objects by the vision module. We receive from the vision module a matrix (containing ‘1s’ or ‘0s’) defining if the pixel is an obstacle or not. We also receive the start and goal points. For global navigation, we chose the A* algorithm. Take a look at a typical result of the A*star algorithm in our program."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96402da0",
   "metadata": {},
   "source": [
    "![Astar_result_video](images/Astar_result_video.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7aa3efba",
   "metadata": {},
   "source": [
    "In this algorithm we will find the shortest path to goal, and we will memorize in memory the total path and the checkpoints of the path. The checkpoints are points in path that are turning points, only theses points are important, as they are going to be passed to motor control. The starting angle of the Thymio will also be passed. To take into account the real size of the Thymio and of the obstacles, the algorithm has scaling factor memorized (the size of a grid point in milimeters). Note that: We also made the obstacles bigger to take into account the size of the Thymio.<br/><br/>\n",
    "The choice of the A* algorithm is mainly due to its completeness and optimality. It is well suited for this project as the camera provide a discrete map and its easy to implement. Others global navigation methods were presented in the course but were not chosen for several reasons. Visibility maps or cell decomposition maps are quite complex and will not give better results for this project. Potential fields maps are easy to understand but at a cost that the robot can be trapped in a local minima and never reach the goal. One major drawback of the A* is that it’s quite computationally heavy for very large maps and tends to be very slow.<br/><br/> \n",
    "Several timing testing of the A* algorithm gave a good estimate of maximum size of map (50x50) for fast running algorithms. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87a9f58f",
   "metadata": {},
   "source": [
    "Here's a recap of some of the timer tests:\n",
    "\n",
    "| dimensions  | Map without growth obstacles[s] | Map with growth Obstacles[s] |\n",
    "|:---------------------------:|:---------------------:|:-------------------------------:|\n",
    "|            50x50            |           2.25        |           1.87          |\n",
    "|           100x100           |         43.3         |           59.48           |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa4ff6e4",
   "metadata": {},
   "source": [
    "Complexity increase dramatically with the size of the map, the cost of the main function in time order for a 100x100:\n",
    "1. A star algorithm (54s)\n",
    "2. Calculating checkpoints(3.88s)\n",
    "3. Plotting(1.43s) \n",
    "4. Growing the obstacles(0.17s)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "979e57dd",
   "metadata": {},
   "source": [
    "Knowing that, all elements from the camera analysis must then be downsized to maximum (50x50) in order to get a fast enough A* algorithm. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84c8efc2",
   "metadata": {},
   "source": [
    "## 5.Control law"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22eeda22",
   "metadata": {},
   "source": [
    "The control law of the Thymio can be split into five states whose simplified operation is given in the Finite State Machine below. <br/><br/>\n",
    "**States**: <br/>\n",
    "Starting : The robot waits until it is detected by the camera and as soon as this is the case, it initializes the first checkpoint as the current checkpoint aim of the robot. <br/>\n",
    "Afterwards, it switches to the Rotating state in order to angle itself in a proper manner to get to the current checkpoint aimed at. <br/> <br/>\n",
    "Rotating : The robot rotates until it aligns with the direction of the target checkpoint it wants to reach. In order to have an overall smoother motion, we leave a margin of error using a threshold such that when the difference between the estimated angle and the wanted angle is lower than this value, then we switch to the Moving state. <br/> <br/>\n",
    "Moving : The robot advances in a straight line towards the checkpoint it is aiming at (current checkpoint), we make use of a distance threshold such that we increment to the next checkpoint if the distance between the thymio and the current checkpoint is below the value. <br/> \n",
    "If it has deviated from its direction it returns to the Rotating state. <br/>\n",
    "As soon as it has arrived at the target checkpoint, two cases are to be differentiated : <br/>\n",
    "-\tIt is not the last checkpoint, then it changes its target checkpoint (increments the new_checkpoint_index by one) and returns to the Rotating state <br/> \n",
    "-\tIt is the last checkpoint, then it goes to Stop state  <br/> <br/>\n",
    "Stop : The Stop state stops the robot when the end goal has been reached. We also set a bool to true which will allow us to end the timer running the program. <br/> <br/>\n",
    "Obstacle avoidance : If an obstacle is detected by one of the horizontal proximity sensors of the robot, it starts to rotate in the clockwise direction until none of the sensors detect any obstacle, we also make use of a threshold here. <br/> \n",
    "It then returns to the rotating state which will try to direct it to the next checkpoint and so on until it has passed the obstacle (see figure) <br/> <br/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b324c82",
   "metadata": {},
   "source": [
    "![Obstacle_avoidance.drawio](images/Obstacle_avoidance.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86eaf6d1",
   "metadata": {},
   "source": [
    "### Inputs and outputs\n",
    "The control law needs 5 inputs : <br/> <br/>\n",
    "sensor_data: array containing the values of the 5 front horizontal proximity sensors <br/> <br/>\n",
    "checkpoints_data: set/array containing all the checkpoints the thymio has to go through, each row is a checkpoint <br/> <br/>\n",
    "fsm_state: indicates what state of the fsm we are in (\"starting\", \"moving\", \"rotating\", \"obstacle_avoidance\", \"stop\") <br/> <br/>\n",
    "state_kalman: state estimate from kalman <br/> <br/> <br/> \n",
    "And return 4 output : <br/> <br/>\n",
    "left_velocities and right_velocities  : velocity of left/right wheel as decided by control law in metric [mm/s] <br/> <br/> \n",
    "new_fsm_state: updated fsm_state <br/> <br/>\n",
    "new_checkpoint_index: updated checkpoint index <br/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "069c6ccf",
   "metadata": {},
   "source": [
    "![Control_Law](images/Control_Law.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5068965a",
   "metadata": {},
   "source": [
    "## 6.Kalman filter"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d9972e0",
   "metadata": {},
   "source": [
    "For the extended Kalman filter, we first needed to model the way the Thymio moves with respect to it’s motor speeds. Looking at the image below, we see how we reference the position and angle of the Thymio. <br/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b00994c",
   "metadata": {},
   "source": [
    "![KalmanFilter](images/KalmanFilter.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fc45336",
   "metadata": {},
   "source": [
    "We end up with a nonlinear function to update the position and state as can be seen in the equation below :"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d4caf2a",
   "metadata": {},
   "source": [
    "![Kalman](images/Kalman.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89645756",
   "metadata": {},
   "source": [
    "Then following the usual extended Kalman filter algorithm, we implement it in Python. <br/>\n",
    "Now the way it works is that whenever the camera is available and is able to find the Thymio on the map, then we make use of that as a measurement for the correction step. Otherwise when the camera isn’t available we only use the update step using the odometry. <br/>\n",
    "We have also added some protective measures to use directly the camera estimation instead when we have huge spikes in the estimations which wouldn’t make sense. <br/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7c10a0e",
   "metadata": {},
   "source": [
    "## 7.Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4fd15da",
   "metadata": {},
   "source": [
    "This project has been a great way for us to apply the different concepts seen in the course that it is the vision, the global and local navigation as well as the uncertainty management (Kalman filter). It has also been the opportunity to see the importance and the difficulty of the pooling of independently developed modules even when they are jointly thought upstream. <br/><br/> \n",
    "To conclude, even if we had some difficulties with the Kalman filter or the camera, we think we have succeeded in making a project that shows the different concepts of the course and fits in our project specifications"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ce625c1",
   "metadata": {},
   "source": [
    "## 8. References"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c23eaef8",
   "metadata": {},
   "source": [
    "[1: https://www.thymio.org/fr/] <br/>\n",
    "[2 : https://en.wikipedia.org/wiki/Canny_edge_detector] <br/>\n",
    "[3 : https://jdhao.github.io/2019/02/23/crop_rotated_rectangle_opencv] <br/>\n",
    "[4 : https://fr.wikipedia.org/wiki/HSV] <br/>\n",
    "[5 : https://www.youtube.com/watch?v=tk9war7_y0Q] <br/>\n",
    "[6 : https://en.wikipedia.org/wiki/Extended_Kalman_filter]<br/>\n",
    "[7 : https://moodle.epfl.ch/mod/resource/view.php?id=1090142]<br/>\n",
    "[8 : https://docs.opencv.org/3.4/d2/d96/tutorial_py_table_of_contents_imgproc.html]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59ef1fea",
   "metadata": {},
   "source": [
    "## 9. Test run of the whole project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c012f39",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-12T19:53:54.852439Z",
     "start_time": "2021-12-12T19:53:54.585509Z"
    }
   },
   "outputs": [],
   "source": [
    "%run src\\project\\Global_nav.py\n",
    "%run src\\project\\DetectionFcts.py\n",
    "%run src\\project\\Locomotion.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f80c324",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-12T19:53:59.004658Z",
     "start_time": "2021-12-12T19:53:56.514860Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from tdmclient import ClientAsync\n",
    "client = ClientAsync()\n",
    "node = await client.wait_for_node()\n",
    "await node.lock()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "332c8150",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-12T19:54:00.770670Z",
     "start_time": "2021-12-12T19:54:00.760915Z"
    }
   },
   "outputs": [],
   "source": [
    "def main(checkpoints_data,m, contours, area_max, Q, R, Ts, webcam) :\n",
    "    global current_checkpoint_index, fsm_state, cam_available, cam_data, x_est_prev, P_est_prev, reached_end, speed\n",
    "    \"\"\"\n",
    "    Main function which coordinates the whole application\n",
    "    \n",
    "    global : takes the variables that will be changed throughout the application\n",
    "    \n",
    "    param checkpoints_data: velocity of left wheel as decided by control law in metric [mm/s]\n",
    "    param Q: Q matrix for the Kalman Filter\n",
    "    param R: R matrix for the Kalman Filter\n",
    "    param Ts: Period for the Python thread which uses the main(...) function\n",
    "    \n",
    "    no return \n",
    "    \"\"\"\n",
    "    if webcam:\n",
    "        success,img = cap.read()\n",
    "        cam_available = True\n",
    "    else: \n",
    "        cam_available = False\n",
    "    \n",
    "    if cam_available:\n",
    "        Thymio_coord, Thymio_found = analyse_thymio(img,contours, area_max, m.ratio_downscale)\n",
    "    \n",
    "    if (Thymio_found):\n",
    "        corrected_estimate_angle = -Thymio_coord[2]\n",
    "        if corrected_estimate_angle < 0 :\n",
    "            corrected_estimate_angle = corrected_estimate_angle +2*math.pi\n",
    "        Thymio_coord = (Thymio_coord[0]*m.ratio_total, (m.height-Thymio_coord[1])*m.ratio_total, corrected_estimate_angle)\n",
    "    else:\n",
    "        Thymio_coord = np.array([0,0,0])\n",
    "    [x_est_prev, P_est_prev] = kalman_filter(speed, Thymio_found, Thymio_coord, x_est_prev, P_est_prev, Q, R, Ts)\n",
    "    \n",
    "    sensor_data = np.array(node.v.prox.horizontal[0:5]) #get data from horiz proximity sensors\n",
    "    \n",
    "    [left_velocity, right_velocity, fsm_state, current_checkpoint_index, reached_end] = control_law(sensor_data, checkpoints_data, current_checkpoint_index, fsm_state, x_est_prev, Thymio_found)\n",
    "    print('Checkpoint number : '+str(current_checkpoint_index) + '\\n')\n",
    "    print('FSM state : '+str(fsm_state) + '\\n \\n')\n",
    "    \n",
    "    speed = [left_velocity, right_velocity]\n",
    "    \n",
    "    node.send_set_variables(motors(left_velocity, right_velocity))\n",
    "    cam_avaiable = False \n",
    "    Thymio_found = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4fd1818",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-12T19:54:58.190865Z",
     "start_time": "2021-12-12T19:54:06.755579Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import math\n",
    "from statistics import mean\n",
    "import numpy as np\n",
    "import cv2\n",
    "# import matplotlib.pyplot as plt\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "\n",
    "# Camera intialisation\n",
    "webcam = True\n",
    "cap = cv2.VideoCapture(1)\n",
    "cap.open(1, cv2.CAP_DSHOW)\n",
    "\n",
    "# While loop to get a proper first image\n",
    "while True:\n",
    "    if webcam:\n",
    "        success,img = cap.read()\n",
    "        plt.figure(figsize=(7, 7))\n",
    "        rgb_img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "        plt.imshow(rgb_img)\n",
    "        plt.show()\n",
    "        \n",
    "    keypress1 = input(\"Satisfying Image ? : \")\n",
    "    if keypress1 ==\"y\":\n",
    "        m, contours, area_max = analyse(img)\n",
    "        keypress2 = input(\"Satisfying analysis ? : \")\n",
    "        if keypress2 ==\"y\":\n",
    "            break\n",
    "\n",
    "### Control Law parameters intialization\n",
    "checkpoints_data = m.checkpoints.transpose()*m.ratio_total # checkpoints converted to mm\n",
    "length_checkpoint = np.size(checkpoints_data, 0)\n",
    "for i in range(length_checkpoint):\n",
    "    # adjusting checkpoints due to different references\n",
    "    checkpoints_data[i][1] = m.height*m.ratio_total - checkpoints_data[i][1] \n",
    "\n",
    "current_checkpoint_index = 0\n",
    "fsm_state = \"starting\"\n",
    "left_velocity = 0\n",
    "right_velocity = 0 \n",
    "speed = [0, 0]\n",
    "\n",
    "### Kalman filter parameters initalization\n",
    "corrected_start_angle = -m.start[2]\n",
    "if corrected_start_angle < 0 :\n",
    "    corrected_start_angle = corrected_start_angle + 2*math.pi\n",
    "    \n",
    "x_init = np.array([m.start[0]*m.ratio_total, (m.height-m.start[1])*m.ratio_total, corrected_start_angle])\n",
    "P_init = np.diag([0.45, 0.45, 0.4])\n",
    "x_est_prev = x_init \n",
    "P_est_prev = P_init\n",
    "\n",
    "Q = np.diag([0.3, 0.3, 0.5])                  \n",
    "R = np.diag([0.7, 0.7, 0.9])\n",
    "\n",
    "### End of application bool\n",
    "reached_end = False\n",
    "\n",
    "### Thread parameter\n",
    "Ts = 0.2\n",
    "\n",
    "timer = RepeatedTimer(Ts, main, checkpoints_data,m, contours, area_max, Q, R, Ts, webcam) # it auto-starts, no need of rt.start()   \n",
    "\n",
    "try:\n",
    "    while not(reached_end) : \n",
    "        await node.wait_for_variables() # wait for Thymio variables values\n",
    "        await client.sleep(1)\n",
    "finally:\n",
    "    timer.stop() \n",
    "    print('Application has finished, bye bye !')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd68bee4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-12T19:55:10.352031Z",
     "start_time": "2021-12-12T19:55:10.229830Z"
    }
   },
   "outputs": [],
   "source": [
    "#Now unlock the robot:\n",
    "await node.unlock()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ace9c5a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
